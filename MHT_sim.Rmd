---
title: "MHT Simulation"
author: "Connor Demorest"
date: "9/4/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, fig.width = 10, fig.height = 5, 
                      cache = TRUE)
library(tidyverse)
library(magrittr)
library(mvtnorm)
```

<!-- Simulation study:  -->

<!--   1) Take 100 samples from N(0,1) and N(5,1) mixed with proportion p in (0,1). Take $\alpha = 0.05$. $X_i = (1-p)*N(0,1) + p*N(3,1)$ -->

<!--   2) Find p values of X -->

<!--   3) Use BH, etc  -->

<!--   4) Get FDP and Power -->

<!--   5) Repeat many times to find FDR and avg power across values of p -->

# Background

To generate fake data that have a known $pi$, the proportion of true alternative hypotheses (or false null hypotheses), $m$ random $Bernoulli(\pi)$ variables are created. This gives the "truth" statement for each alternative hypothesis. The null hypotheses are distributed Normal(0, 1) and the alternative hypotheses have $Normal(\mu, 1)$ distribution, creating a bi-modal mixture distribution with the density in each mode depending on the value of $pi$, shown in Figure 1. 

```{r, eval = T}
p = 0.4
a = 0.05
num_samples = 100
# True if true mean =/= 0
## A = c(rep(1, p*num_samples), rep(0, (1-p)*num_samples))
set.seed(1234)
A = rbernoulli(n = num_samples, p = p)
t_nulls = rnorm(num_samples, 0, 1)
f_nulls = rnorm(num_samples, 3, 1)
samples = ifelse(A, f_nulls, t_nulls)
p_vals = pnorm(samples, lower.tail = F)

ggplot() + 
  geom_density(aes(x = samples, color = "Sim data density", lty = "Sim data density"),
               lwd = 1) + 
  stat_function(fun = function(x) (0.4*dnorm(x, mean = 3, sd = 1) + 0.6*dnorm(x, 0, 1)), 
                aes(col = "Theoretical mixture", lty = "Theoretical mixture"),
                lwd = 1) +
  xlim(-4, 7) +
  theme_bw() + 
  labs(x = "",
       y = "Density",
       title = "Figure 1: Empirical vs theoretical mixture of Normal distributions",
       caption = "m=100 simulations, 60% from N(0,1) and 40% from N(3,1) distributions.",
       color = "",
       lty = "") +
  theme(plot.title = element_text(hjust = 0.5))
```

After creating the mixture distribution, the $Pr(X > x | \mu = 0)$ (the p-value) is calculated. The p-values are corrected for multiplicity errors using the Bonferroni method and the Benjamini-Hochberg (BH) method. The unadjusted (no correction) and the Bonferroni and BH corrected p-values are used to determine the proportion of rejections of the null hypothesis when the null hypothesis is false (power) and the proportion of incorrect rejections of the null hypothesis (the Type 1 error) divided by the the total proportion of rejections of the null hypothesis (called "discoveries" from now on). This proportion calculated by $FDP = \frac{\text{\# incorrect rejections}}{\text{Total \# rejections}}$ is the false discovery proportion (FPD). The expected value of the FDP in the long run is the false discovery rate (FDR). 

\newpage

# Simulation studies for BH and Lfdr

### Study 1: Benjamini-Hochberg method compared to Bonferroni correction and no adjustment on FDR, independent case

1) Generate $\theta_i$ for $i = 1, 2, ..., m$ from an iid random sample of $Ber(\pi_1)$, where $\pi_1$ is the probability of $\theta = 1$, and $\pi_0$ is the probability of $\theta = 0$, $\pi_0 + \pi_1 = 1$.
2) For each $\theta_i = 1$, draw from a $f_1 = N(\mu, 1)$ and for $\theta_i = 0$ draw from $f_0 = N(0, 1)$. Let $X_i = \theta_i*f_1 + (1-\theta_i)*f_0$. 
3) Calculate the p-value $Pr(X > x | \mu = 0)$ for each $X_i$ testing $H_0: \mu = 0$ against $H_0: \mu > 0$, and apply each correction procedure at $\alpha = 0.05$ level. Track the proportion of rejections of the null hypothesis when the null hypothesis is true (false discovery proportion) and the proportion of rejecting the null hypothesis when the null hypothesis is false (Power). 
4) Repeat 1-3 50 times for each of the correction methods, for values of $\pi_1 = 0.01\ to\ 0.99$, $m = \{30, 100\}$ and $\mu = \{3, 5\}$.

<!-- To simulate the properties of the power and the FDR, a function that takes the proportion of false null hypotheses $pi$, the number of hypotheses $m$, and the alternative hypothesis `alt_hyp` as values was used to calculate the long run behavior under each combination for each method of controlling the FDR in a simulation study.  -->

<!-- For each combination of $pi$, $m$, `alt_hyp`, and method, 50 simulations was used to calculate the power and the FDR. The results are plotted in Figures 2 and 3 below. The FDR when using the BH method is controlled at the 5% level no matter what the parameters of the simulation are, which allows the power to increase drastically over the Bonferroni method, and does not change with increases in the number of hypotheses. The Bonferroni method is much too conservative in that it doesn't allow enough rejections of the null hypothesis to occur, which limits the power and the ability to scale with larger $m$. On the other hand, not controlling for the multiplicity corrections allows for much higher power, but at the cost of rejecting the null hypothesis far too often and having a much too high FDR.  -->

<!-- Talk about how the methods work? -->

```{r, eval = T, warning = F}
# m === number of total hypotheses
func = function(p = 0.4, a = 0.05, m = 10, alt_hyp = 3, method = "BH") {
  # # A === True discoveries
  # p = 0.4
  # a = 0.05
  # m = 10
  # alt_hyp = 3
  # method = "BH"
  A = rbernoulli(m, p)
  # t_nulls = rnorm(m, 0, 1)
  # f_nulls = rnorm(m, alt_hyp, 1)
  # samples = ifelse(A, f_nulls, t_nulls)
  matp = MASS::mvrnorm(n = 1, 
                 mu = ifelse(A, alt_hyp, 0),
                 Sigma = diag(length(A)))
  
  p_vals = pnorm(matp, lower.tail = F)
  # p_vals = pnorm(samples, lower.tail = F)
  adj_p_vals = p.adjust(p = p_vals, method = method)
  
  # Test discoveries
  B = (adj_p_vals <= a)
  # FP = A' & B
  False_positive = !A & B
  # FDP = P(A' & B)/P(B)
  FDP = ifelse(is.na(sum(!A & B)/sum(B)), 0, sum(!A & B)/sum(B))
  
  # Power = P(reject Null | Null false) = P(reject & Null false)/P(Null false)
  Power = ifelse(is.na(sum(A & B)/sum(A)), 0, sum(A & B)/sum(A))
  return(c(FDP, Power))
}
# New function: do.many, where it just does 'many' and returns averages?
# Calculate FDR and Avg Power

# many = replicate(1000, func(method = "none")) %>% apply(., 1, FUN = mean) %>% print

df = expand_grid(p = rep(seq(0.01, 0.7, length.out = 10), each = 50), 
                 method = c("BH", "bonferroni", "none"),
                 alt_hyp = c(3,5), 
                 m = c(30, 100)) 

parallel::mcmapply(func,
                   mc.cores = parallel::detectCores(),
                   p = df$p, method = df$method, 
                   alt_hyp = df$alt_hyp, 
                   m = df$m) %>% 
  t() %>% 
  as_tibble() %>% 
  rename(FDR = V1, Power = V2) %>%
  mutate(df) -> many

# Increased p gives more true discoveries
ggplot(data = many, aes(x = p, y = Power, color = method, linetype = method)) +
  geom_point(size = 0.3, alpha = 0.2) +
  stat_summary(fun = "mean", geom = "line") +
  facet_grid(alt_hyp ~ m) +
  labs(x = expression("Proportion of false null hypotheses," ~ pi[1]),
       title = "Figure 2: Power of variations of parameters for FDR control methods") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "True Difference in Means",
                                         breaks = NULL, labels = NULL), limits = c(0,1)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses",
                                         breaks = NULL, labels = NULL))

ggplot(data = many, aes(x = p, y = FDR, color = method, linetype = method)) + 
  geom_point(size = 0.3, alpha = 0.3) +
  stat_summary(fun = "mean", geom = "line") + 
  facet_grid(alt_hyp ~ m) +
  geom_abline(intercept = 0.05, slope = 0) + 
  labs(x = expression("Proportion of false null hypotheses," ~ pi[1]),
       title = "Figure 3: FDR for variations of parameters for FDR control methods") +
  coord_cartesian(ylim =  c(0, 0.3)) + 
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "True Difference in Means", 
                                         breaks = NULL, labels = NULL)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses", 
                                         breaks = NULL, labels = NULL))
```


<!-- :::Not Run::: -->
```{r, warning = F, eval = F}
# Basically re-using code from above
# p known here!
p = 0.4
alpha = 0.05
num_samples = 1000
## True if true mean =/= 0
# theta = rbernoulli(num_samples, p)
# f0 = rnorm(num_samples, 0, 1)
# f1 = rnorm(num_samples, 5, 1)
# X = ifelse(theta, f1, f0)
# f = (1-p)*dnorm(X, 0, 1) + p*dnorm(X, 5, 1)

# Lfdr = (1-pi)f0(X)/f(x)
PFDR_fun = function(p = 0.4, alpha = 0.05, num_samples = 1000, alt_hyp = 4) {
  df = tibble(theta = rbernoulli(num_samples, p),
              X = ifelse(theta, rnorm(num_samples, alt_hyp, 1), rnorm(num_samples, 0, 1)),
              f = (1-p)*dnorm(X, 0, 1) + p*dnorm(X, alt_hyp, 1),
              Lfdr = (1-p)*dnorm(X, 0, 1)/f) %>% 
    arrange(Lfdr) %>% 
    mutate(q = cumsum(Lfdr)/(1:length(Lfdr)), 
           `q<a` = q < alpha, # Total discoveries
           Type1error = `q<a`&!theta, # Type 1 errors
           Type2error = theta&!`q<a`) %>% # Type 2 errors
    summarise(theta = sum(theta), disc = sum(`q<a`), t1 = sum(Type1error), t2 = sum(Type2error), p, alt_hyp, num_samples)
  return(df)
}
# PFDR_fun()
# reps = plyr::rdply(100, PFDR_fun(), .id = NULL) %>% summarise(PFDR = t1/disc)
# 
# mean(as.matrix(reps))

#### Old code ####
# PFDR = rep(NA, length(Lfdr))
# Q = rep(NA, length(Lfdr))
# for(t in 1:length(Lfdr)) {
#   Q[t] = (sum(sort(Lfdr)[1:t]/t))# %>% print
#   PFDR[t] = ((sum(sort(Lfdr)[1:t]/t)) < alpha)# %>% print
# }
##### 

pr = seq(0.01, 0.99, length.out = 20)
alt = c(2, 4)
samps = c(30, 100, 500)
pr_alt_samps = expand.grid(pr, alt, samps)

# If no discoveries, then no type 1 errors and fdr = 0
reps = replicate(100, 
                 mapply(PFDR_fun, 
                        p = pr_alt_samps[,1], 
                        alt_hyp = pr_alt_samps[,2], 
                        num_samples = pr_alt_samps[,3]) %>% t(), 
                 simplify = F) %>% 
  do.call(rbind, .) %>% 
  as_tibble() %>% 
  mutate(across(.col = everything(), as.double),
         PFDR = ifelse(is.na(t1/disc), 0, t1/disc),
         Power = 1 - ifelse(is.na(t2/theta), 1, t2/theta))

# ### Make plots! Change values of p, 10 are plenty
# ggplot(reps, mapping = aes(x = p, y = PFDR)) + 
#   geom_point(size = 0.3, alpha = 0.3, mapping = aes(color = "FDR")) +
#   geom_hline(yintercept = alpha, color = "blue") +
#   stat_summary(fun = "mean", geom = "line", mapping = aes(color = "FDR")) +
#   geom_point(size = 0.3, alpha = 0.3, 
#              mapping = aes(x = p, y = Power, color = "Power")) +
#   stat_summary(fun = "mean", geom = "line", 
#                mapping = aes(x = p, y = Power, color = "Power")) +
#   facet_grid(alt_hyp ~ num_samples) +
#   geom_abline(intercept = 0.05, slope = 0) + 
#   labs(x = "Proportion of false null hypotheses",
#        title = "Lfdr control method simulation",
#        y = "",
#        color = "") +
#   scale_y_continuous(sec.axis = sec_axis(~ . , name = "True Difference in Means", 
#                                          breaks = NULL, labels = NULL), limits = c(0,1)) +
#   scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses", 
#                                          breaks = NULL, labels = NULL), limits = c(0,1))

ggplot(reps, mapping = aes(x = p, y = Power)) + 
  # geom_point(size = 0.3, alpha = 0.3, mapping = aes(color = "FDR")) +
  # geom_hline(yintercept = alpha, color = "blue") +
  # stat_summary(fun = "mean", geom = "line", mapping = aes(color = "FDR")) +
  geom_point(size = 0.3, alpha = 0.3) +
  stat_summary(fun = "mean", geom = "line") +
  facet_grid(alt_hyp ~ num_samples) +
  labs(x = "Proportion of false null hypotheses",
       title = "Lfdr control method simulation",
       y = "Power") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "True Difference in Means", 
                                         breaks = NULL, labels = NULL), limits = c(0,1)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses", 
                                         breaks = NULL, labels = NULL), limits = c(0,1))

### Make plots! Change values of p, 10 are plenty
ggplot(reps, mapping = aes(x = p, y = PFDR)) +
  geom_point(size = 0.3, alpha = 0.3) +
  geom_hline(yintercept = alpha, color = "blue") +
  stat_summary(fun = "mean", geom = "line") +
  facet_grid(alt_hyp ~ num_samples) +
  geom_abline(intercept = 0.05, slope = 0) +
  labs(x = "Proportion of false null hypotheses",
       title = "Lfdr control method simulation",
       y = "",
       color = "") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "True Difference in Means",
                                         breaks = NULL, labels = NULL), limits = c(0,0.3)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses",
                                         breaks = NULL, labels = NULL), limits = c(0,1))
```
<!-- ::: Not Run::: -->


<!-- Using the same setting as the Lfdr procedure, compare the performance of the Lfdr and the BH procedure,  -->

<!-- a) When the variances are same, i.e., 1 in both null and non-null groups _-> see above_ -->

<!-- b) Draw the samples from a multivariate normal distribution. For example, if you have 100 samples, out of which $\pi =0.5$ (say), proportion are from the non-null distribution, draw one sample from the following: -->
<!--   Define mu_vector = (1-theta)f(0) + (theta)f(2), i.e. a 100x1 vector of 0 (null mean) and 2(non-null mean). -->
<!--   Sigma = a 100x100 matrix, with diagonal elements 1 and off diagonal elements = rho (a positive value between 0 and 1). -->
<!--  Draw one sample from MVNorm_100($\mu$ = mu_vector, $\sigma$ = Sigma) -->
<!-- Find the p-values, and Apply the BH and Lfdr process on this sample to figure out the discoveries, false and true discoveries. -->
<!-- This would show the performance of the two procedures when the p-values are positively dependent. -->

\newpage

# Study 2: Local fdr comparison to BH under PRDS assumption

1) Generate $\theta_i$ for $i = 1, 2, ..., m$ from an iid random sample of $Ber(\pi_1)$, where $\pi_1$ is the probability of $\theta = 1$, and $\pi_0$ is the probability of $\theta = 0$, $\pi_0 + \pi_1 = 1$.
2) Sample a vector $X$ of length $m$ draws from a multivariate Normal distribution with mean $\mu$ when $\theta_i = 1$ and mean 0 when $\theta_i = 0$, and covariance matrix of 1 on the diagonals and $\rho$ on the off-diagonals. Let $f(X) = \pi_0*f_0(X) + \pi_1*f_1(X)$ be the "mixture density", where $f_0(X) = N(0, 1)$ and $f_1(X) = N(\mu, 1)$. 
3) Calculate the p-value $Pr(X > x | \mu = 0)$ for each $X_i$ and adjust using the BH method. 
4) Calculate the $lfdr(X) = \pi_0*f_0(X)/f(X)$ for each $X_i$. 
5) Order the $lfdr(X)$ from smallest to largest, reject the null hypothesis for the first $k$ ordered hypotheses where $\frac{1}{k}\sum_{i = 1}^{k}lfdr(X_{(i)}) < \alpha$ and all BH adjusted p-values $p < \alpha$. Track the proportion of rejections of the null hypothesis when the null hypothesis is true (false discovery proportion) and the proportion of rejecting the null hypothesis when the null hypothesis is false (Power) for both Local FDR and BH FDR adjustment methods. 
6) Repeat 1-5 for 50 replicates for values of $\pi_1 = 0.01\ to\ 0.99$, $rho$ = 0 (independent case), $\mu = \{3,5\}$, $m = \{30, 100, 250\}$, and again for values of $\pi_1 = 0.01\ to\ 0.7$, $rho = \{0, 0.1, 0.8\}$ (positive dependent case), $\mu = 3$, $m = \{30, 100, 250\}$.

\newpage

<!-- Independent case: -->
```{r}
# Takes in rho, N, proportions, and the alternative hypothesis
# Outputs the T1 and T2 error rates for BH and Lfdr
BH_lfdr_fun = function(num.samples = 100, p = 0.4, rho = 0, alt.hyp = 3) {
  # Sigma has correlation structure rho and 1 on diagonal
  Sigma = matrix(nrow = num.samples, ncol = num.samples, data = rho)
  diag(Sigma) = 1
  # For known p, simulate the true hypothesis state
  theta = rbernoulli(num.samples, p)
  # Generate positively correlated data
  MVN = MASS::mvrnorm(n = 1, mu = ifelse(theta, alt.hyp, 0), Sigma = Sigma)
  # Get p-values for BH and adjust them
  MVN_p = pnorm(MVN, mean = 0, sd = 1, lower.tail = F)
  BH_p = p.adjust(MVN_p, method = "BH")
  # Copy/paste code from before to get Lfdr
  df = tibble(theta, 
              BH_p,
              f = (1-p)*dnorm(MVN, 0, 1) + p*dnorm(MVN, alt.hyp, 1),
              Lfdr = (1-p)*dnorm(MVN, 0, 1)/f) %>%
    arrange(Lfdr) %>%
    mutate(q = cumsum(Lfdr)/(1:length(Lfdr)),
           `q<a` = q < 0.05,
           `BH<a` = BH_p < 0.05,
           # Calculate errors for BH and Lfdr methods
           Lfdr_Type1error = `q<a`&!theta,
           Lfdr_Type2error = !`q<a`&theta,
           BH_Type1error = `BH<a` & !theta,
           BH_Type2error = theta & !`BH<a`) %>% 
    summarise(theta = sum(theta),
              Lfdr_disc = sum(`q<a`),
              BH_disc = sum(`BH<a`),
              Lfdr_T1 = sum(Lfdr_Type1error),
              Lfdr_T2 = sum(Lfdr_Type2error),
              BH_T1 = sum(BH_Type1error),
              BH_T2 = sum(BH_Type2error),
              p, rho, num.samples, alt.hyp)
  return(df)
}
# Independent case: rho = 0
df1 = expand.grid(p = rep(seq(0.01, 0.99, length.out = 20), times = 50), 
                 num.samples = c(30, 100, 250),
                 alt.hyp = c(3, 5))

parallel::mcmapply(FUN = BH_lfdr_fun, 
                   mc.cores = parallel::detectCores(), 
                   p = df1$p, 
                   num.samples = df1$num.samples, 
                   rho = 0, 
                   alt.hyp = df1$alt.hyp) -> b

# Move data around into a nice way to plot using ggplot
BH_Lfdr1 = t(b) %>% 
  as_tibble(column_name = rownames(b)) %>% 
  mutate_all(as.double) %>%
  mutate(BH_FDR = BH_T1/pmax(BH_disc, 1),
         Lfdr_FDR = Lfdr_T1/pmax(Lfdr_disc, 1),
         BH_Power = ifelse(theta == 0, 0, 1-BH_T2/theta),
         Lfdr_Power = ifelse(theta == 0, 0, 1-Lfdr_T2/theta), 
         .keep = "unused") %>%
  pivot_longer(cols = contains("_"),
               names_to = c("Method", "Type"),
               names_sep = "_",
               values_to = "Prob")

# Plot things
ggplot(BH_Lfdr1 %>% filter(Type == "FDR"), 
       mapping = aes(x = p, y = Prob, color = Method, linetype = Method)) + 
  stat_summary(fun = "mean", geom = "line") +
  geom_point(size = 0.3, alpha = 0.3) +
  geom_hline(yintercept = 0.05) + 
  facet_grid(alt.hyp ~ num.samples) +
  labs(x = expression("Proportion of false null hypotheses," ~ pi[1]),
       title = "FDR of Lfdr vs BH control methods in independence",
       y = "False discovery rate") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Alternative Hypothesis", 
                                         breaks = NULL, labels = NULL)) +
  coord_cartesian(ylim = c(0, 0.2)) + 
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses", 
                                         breaks = NULL, labels = NULL))

ggplot(BH_Lfdr1 %>% filter(Type == "Power"), 
       mapping = aes(x = p, y = Prob, color = Method, linetype = Method)) + 
  geom_point(size = 0.3, alpha = 0.3) +
  stat_summary(fun = "mean", geom = "line") +
  facet_grid(alt.hyp ~ num.samples) +
  labs(x = expression("Proportion of false null hypotheses," ~ pi[1]),
       title = "Power of Lfdr vs BH control methods in independence",
       y = "Power") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Alternative Hypothesis", 
                                         breaks = NULL, labels = NULL), limits = c(0,1)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses", 
                                         breaks = NULL, labels = NULL))
```

\newpage

```{r}
# PRDS case: rho =/= 0
df2 = expand.grid(p = rep(seq(0.01, 0.99, length.out = 10), times = 50),
                 num.samples = c(30, 100, 250),
                 rho = c(0, 0.1, 0.8)) 

# Slow as heck without running in parallel
parallel::mcmapply(FUN = BH_lfdr_fun, 
                   mc.cores = parallel::detectCores(), 
                   p = df2$p, 
                   num.samples = df2$num.samples, 
                   rho = df2$rho, 
                   alt.hyp = 3) -> a
# Move data around into a nice way to plot using ggplot
BH_Lfdr2 = t(a) %>% 
  as_tibble(column_name = rownames(a)) %>% 
  mutate_all(as.double) %>%
  mutate(BH_FDR = BH_T1/pmax(BH_disc, 1),
         Lfdr_FDR = Lfdr_T1/pmax(Lfdr_disc, 1),
         BH_Power = ifelse(theta == 0, 0, 1-BH_T2/theta),
         Lfdr_Power = ifelse(theta == 0, 0, 1-Lfdr_T2/theta), 
         .keep = "unused") %>%
  pivot_longer(cols = contains("_"),
               names_to = c("Method", "Type"),
               names_sep = "_",
               values_to = "Prob")

# Plot things
ggplot(BH_Lfdr2 %>% filter(Type == "FDR"), 
       mapping = aes(x = p, y = Prob, color = Method, linetype = Method)) + 
  stat_summary(fun = "mean", geom = "line") +
  geom_point(size = 0.3, alpha = 0.3) +
  geom_hline(yintercept = 0.05) + 
  facet_grid(rho ~ num.samples) +
  labs(x = expression("Proportion of false null hypotheses," ~ pi[1]),
       title = "FDR of Lfdr vs BH control methods in positive dependence",
       y = "False discovery rate") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Amount of correlation", 
                                         breaks = NULL, labels = NULL)) +
  coord_cartesian(ylim = c(0, 0.2)) + 
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses", 
                                         breaks = NULL, labels = NULL))

ggplot(BH_Lfdr2 %>% filter(Type == "Power"), 
       mapping = aes(x = p, y = Prob, color = Method, linetype = Method)) + 
  geom_point(size = 0.3, alpha = 0.3) +
  stat_summary(fun = "mean", geom = "line") +
  facet_grid(rho ~ num.samples) +
  labs(x = expression("Proportion of false null hypotheses," ~ pi[1]),
       title = "Power of Lfdr vs BH control methods in positive dependence",
       y = "Power") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Amount of correlation", 
                                         breaks = NULL, labels = NULL), limits = c(0,1)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses", 
                                         breaks = NULL, labels = NULL))
```

\newpage

```{r}
df3 = expand.grid(rho = rep(seq(0, 0.9, by = 0.1), times = 100),
                 num.samples = c(30, 100, 250),
                 p = c(0.1, 0.4, 0.8)) 

parallel::mcmapply(FUN = BH_lfdr_fun, 
                   mc.cores = parallel::detectCores(), 
                   p = df3$p, 
                   num.samples = df3$num.samples, 
                   rho = df3$rho, 
                   alt.hyp = 3) -> c

# Move data around into a nice way to plot using ggplot
BH_Lfdr3 = t(c) %>% 
  as_tibble(column_name = rownames(c)) %>% 
  mutate_all(as.double) %>%
  mutate(BH_FDR = BH_T1/pmax(BH_disc, 1),
         Lfdr_FDR = Lfdr_T1/pmax(Lfdr_disc, 1),
         BH_Power = ifelse(theta == 0, 0, 1-BH_T2/theta),
         Lfdr_Power = ifelse(theta == 0, 0, 1-Lfdr_T2/theta), 
         .keep = "unused") %>%
  pivot_longer(cols = contains("_"),
               names_to = c("Method", "Type"),
               names_sep = "_",
               values_to = "Prob")

# Plot things
ggplot(BH_Lfdr3 %>% filter(Type == "FDR"), 
       mapping = aes(x = rho, y = Prob, color = Method, linetype = Method)) + 
  stat_summary(fun = "mean", geom = "line") +
  geom_point(size = 0.3, alpha = 0.3) +
  geom_hline(yintercept = 0.05) + 
  facet_grid(p ~ num.samples) +
  labs(x = expression("Correlation between test statistics," ~ rho),
       title = "FDR of Lfdr vs BH control methods in positive dependence",
       y = "False discovery rate") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Proportion of false null hypotheses", 
                                         breaks = NULL, labels = NULL)) +
  coord_cartesian(ylim = c(0, 0.2)) + 
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses", 
                                         breaks = NULL, labels = NULL))

ggplot(BH_Lfdr3 %>% filter(Type == "Power"), 
       mapping = aes(x = rho, y = Prob, color = Method, linetype = Method)) + 
  geom_point(size = 0.3, alpha = 0.3) +
  stat_summary(fun = "mean", geom = "line") +
  facet_grid(p ~ num.samples) +
  labs(x = expression("Correlation between test statistics," ~ rho),
       title = "Power of Lfdr vs BH control methods in positive dependence",
       y = "Power") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Proportion of false null hypotheses", 
                                         breaks = NULL, labels = NULL), limits = c(0,1)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of Hypotheses", 
                                         breaks = NULL, labels = NULL))
```

\newpage

```{r}
df4 = expand.grid(num.samples = rep(seq(10, 250, length.out = 10), times = 100),
                 rho = c(0, 0.1, 0.8),
                 p = c(0.1, 0.4, 0.8)) 

parallel::mcmapply(FUN = BH_lfdr_fun, 
                   mc.cores = parallel::detectCores(), 
                   p = df4$p, 
                   num.samples = df4$num.samples, 
                   rho = df4$rho, 
                   alt.hyp = 3) -> d

# Move data around into a nice way to plot using ggplot
BH_Lfdr4 = t(d) %>% 
  as_tibble(column_name = rownames(d)) %>% 
  mutate_all(as.double) %>%
  mutate(BH_FDR = BH_T1/pmax(BH_disc, 1),
         Lfdr_FDR = Lfdr_T1/pmax(Lfdr_disc, 1),
         BH_Power = ifelse(theta == 0, 0, 1-BH_T2/theta),
         Lfdr_Power = ifelse(theta == 0, 0, 1-Lfdr_T2/theta), 
         .keep = "unused") %>%
  pivot_longer(cols = contains("_"),
               names_to = c("Method", "Type"),
               names_sep = "_",   
               values_to = "Prob")

# Plot things
ggplot(BH_Lfdr4 %>% filter(Type == "FDR"), 
       mapping = aes(x = num.samples, y = Prob, color = Method, linetype = Method)) + 
  stat_summary(fun = "mean", geom = "line") +
  geom_point(size = 0.3, alpha = 0.3) +
  geom_hline(yintercept = 0.05) + 
  facet_grid(rho ~ p) +
  labs(x = expression("Number of hypotheses," ~m),
       title = "FDR of Lfdr vs BH control methods in positive dependence",
       y = "False discovery rate") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Amount of correlation", 
                                         breaks = NULL, labels = NULL)) +
  coord_cartesian(ylim = c(0, 0.2)) + 
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Proportion of signals", 
                                         breaks = NULL, labels = NULL))

ggplot(BH_Lfdr4 %>% filter(Type == "Power"), 
       mapping = aes(x = num.samples, y = Prob, color = Method, linetype = Method)) + 
  geom_point(size = 0.3, alpha = 0.3) +
  stat_summary(fun = "mean", geom = "line") +
  facet_grid(rho ~ p) +
  labs(x = expression("Number of hypotheses," ~m),
       title = "Power of Lfdr vs BH control methods in positive dependence",
       y = "Power") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Amount of correlation", 
                                         breaks = NULL, labels = NULL), limits = c(0,1)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Proportion of signals", 
                                         breaks = NULL, labels = NULL))
```

\newpage

```{r}
df5 = expand.grid(alt.hyp = rep(seq(0.5, 5, length.out = 10), times = 100),
                 rho = c(0, 0.1, 0.8),
                 p = c(0.1, 0.4, 0.8)) 

parallel::mcmapply(FUN = BH_lfdr_fun, 
                   mc.cores = parallel::detectCores(), 
                   p = df5$p, 
                   num.samples = 100, 
                   rho = df5$rho, 
                   alt.hyp = df5$alt.hyp) -> e

# Move data around into a nice way to plot using ggplot
BH_Lfdr5 = t(e) %>% 
  as_tibble(column_name = rownames(e)) %>% 
  mutate_all(as.double) %>%
  mutate(BH_FDR = BH_T1/pmax(BH_disc, 1),
         Lfdr_FDR = Lfdr_T1/pmax(Lfdr_disc, 1),
         BH_Power = ifelse(theta == 0, 0, 1-BH_T2/theta),
         Lfdr_Power = ifelse(theta == 0, 0, 1-Lfdr_T2/theta), 
         .keep = "unused") %>%
  pivot_longer(cols = contains("_"),
               names_to = c("Method", "Type"),
               names_sep = "_",   
               values_to = "Prob")

# Plot things
ggplot(BH_Lfdr5 %>% filter(Type == "FDR"), 
       mapping = aes(x = alt.hyp, y = Prob, color = Method, linetype = Method)) + 
  stat_summary(fun = "mean", geom = "line") +
  geom_point(size = 0.3, alpha = 0.3) +
  geom_hline(yintercept = 0.05) + 
  facet_grid(rho ~ p) +
  labs(x = expression("Mean of alternative hypothesis," ~mu[1]),
       title = "FDR of Lfdr vs BH control methods in positive dependence",
       y = "False discovery rate") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Amount of correlation", 
                                         breaks = NULL, labels = NULL)) +
  coord_cartesian(ylim = c(0, 0.2)) + 
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Proportion of signals", 
                                         breaks = NULL, labels = NULL))

ggplot(BH_Lfdr5 %>% filter(Type == "Power"), 
       mapping = aes(x = alt.hyp, y = Prob, color = Method, linetype = Method)) + 
  geom_point(size = 0.3, alpha = 0.3) +
  stat_summary(fun = "mean", geom = "line") +
  facet_grid(rho ~ p) +
  labs(x = expression("Mean of alternative hypothesis," ~mu[1]),
       title = "Power of Lfdr vs BH control methods in positive dependence",
       y = "Power") +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Amount of correlation", 
                                         breaks = NULL, labels = NULL), limits = c(0,1)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Proportion of signals", 
                                         breaks = NULL, labels = NULL))
```
\newpage

# Code:
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, include = F}
```
